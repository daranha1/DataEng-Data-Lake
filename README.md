# Project : Data Lake

## Author
Diana Aranha

## Purpose:
To enable Sparkify analytics team to continue finding insights in what songs their users are <br/>
listening to.
   
## Current Scenario:
A music startup named Sparkify has grown their user base and song database and want to move<br/>
their data warehouse to a data lake.<br/>  
* Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a<br/> 
  directory with JSON metadata on the songs in their app.
  
## Data Engineer Tasks:
1. Build an ETL pipeline for a data lake hosted on S3.<br/>
2. Extract Data from S3<br/>
3. Transform data into a set of dimensional tables using Spark so Sparkify's analytics<br/> 
   team can continue finding insights in what songs their users are listening to.<br/> 
4. Load the data back into S3 as a set of dimensional tables<br/>
5. Deploy the Spark process on a cluster using AWS<br/>
6. Test the database and ETL pipeline by running queries provided by Sparkify's analytics team<br/>
7. Compare results with expected results of the analytics team.<br/>

## Schema Design
![](https://github.com/daranha1/DataEng-Data-Lake/blob/main/images/Star-Schema-Design-Data-Lake.png)

## Project Datasets
| Data               | Location                     |
|--------------------|------------------------------|
| Song Data          | s3://udacity-dend/song_data  |
| Log Data           | s3://udacity-dend/log_data   |

### Song Dataset
1. This dataset is a subset of real data from the **Million Song Dataset**
2. Each file is in JSON format and contains metadata about a song and the artist of that song.<br/>
3. The files are partitioned by the first three letters of each song's track ID.<br/>
   Filepaths for two files in this dataset are as follows:<br/>
   * **song_data/A/B/C/TRABCEI128F424C983.json**
   * **song_data/A/A/B/TRAABJL12903CDCF1A.json**
   
#### Song file Example : **TRAABJL12903CDCF1A.json**

### Log Dataset
1. The log dataset consists of log files in JSON format generated by an event simulator<br/>
   based on songs in the dataset above.<br/>
   
   **{<br/>
    "num_songs": 1, <br/>
    "artist_id": "ARJIE2Y1187B994AB7", <br/>
    "artist_latitude": null, <br/>
    "artist_longitude": null, <br/>
    "artist_location": "", <br/>
    "artist_name": "Line Renaud", <br/>
    "song_id": "SOUPIRU12A6D4FA1E1", <br/>
    "title": "Der Kleine Dompfaff", <br/>
    "duration": 152.92036, <br/>
    "year": 0 <br/>
   }**
   
#### Log file Example : log_data/2018/11/2018-12-events.json
![](https://github.com/daranha1/DataEng-Data-Lake/blob/main/images/log-data-2018-11-12-events.png)

## Requirements
1. AWS Account
2. Create an IAM Role with Admin Access and full S3 access to allow reading the<br/>
   Udacity input dataset and creating an output S3 bucket programmatically.<br/>
3. Delete files in output S3 bucket and the output S3 bucket once files have been<br/>
   written and analytic queries have been executed to manage costs<br/>
4. The Udacity workspace contains local datasets, Jupyter notebook, python 3.x and Spark software<br/>


|  Local Data    | Files                     |
|----------------|---------------------------|
| Input          | 1. song_data/\*/\*/\*/\*.json |
|                | 2. log_data/*.json        |
| Output         | 1. output/songs           |
|                | 2. output/artists         |
|                | 3. output/time            |
|                | 4. output/users           |
|                | 5. output/songplays       |

| S3 Bucket      | Files                     |
|----------------|---------------------------|
| Input          | 1. udacity-dend/song_data/\*/\*/\*/\*.json    |
|                | 2. udacity-dend/log_data/\*/\*/\*.json |
| Output         | 1. BucketName/songs |
|                | 2. BucketName/artists |
|                | 3. S3-BucketName/time |
|                | 4. S3-BucketName/users |
|                | 5. S3-BucketName/songplays |

| Program Files: | Function                            |
|--------------------|-------------------------------------|
| **etl.py**           | **This python script performs Extract, Transform, Load** |
|                      | 1. Reads the credentials files
|                      | 2. Reads the song_data files and log_data files from local directory or S3 |
|                      | 3. Extracts columns for the fact and dimensional tables |
|                      | 4. Writes files in parquet format and saves to local directory |
|                      | **If the output S3 bucket does not exist a new output bucket in S3 is created|
|                      |            |
| **dl.cfg**           | **Credentials file contains** |
|                      | 1. AWS access keys to gain access to S3 buckets |
|                      | 2. the bucket region                    |
|                      | 3. Input and Output locations for local and S3 buckets |
|                      |    separate bucket for S3 output  |
|                      | **Input and Output S3 buckets must be in the same region |
|                      | otherwise reading and writing to S3 will be very slow** |
|                      |                                |
| **check-parquet-output** | **This Jupyter notebook checks for:** | 
|                      | a. the proper fields for the dimension and fact tables |
|                      | b. performs analysis on the users for free and paid users and for free and | 
|                      |    paid users among male and female subscribers |

| GitHub directories |     Files        |
|-------------|-------------------------|
| /data       | contains song_data and log_data folders with their files   |
| /data/output  | contains parquet files for each fact and dimension table   |
| /screenshots | contains schemas for the fact and dimension tables |
| /images     | contains sample log_data file and star schema diagram |

### Parquet files
1. Dimension tables -- songs, time, users, artists and fact table songplays are written<br/>
   in parquet format to S3.<br/>
2. Songs table files are partitioned by year and artist.<br/>
3. time table files are partitioned by year and month.<br/>
4. Songplays table files are partitioned by year and month.<br/>

## Implementation
**1. Run the Python script etl.py in the Udacity workspace terminal**<br/>
     python etl.py<br/><br/>
**2. The user of this script determines if local files or files on S3 will be used.**<br/>
     2a. Enter 1 for processing local files and saving output to local directory<br/>
     2b. Enter 2 for processing files on S3 and saving output to S3<br/>
     2c. Enter 3 to quit the script<br/><br/>
**3. Run the check-parquet-output.ipynb Jupyter notebook to check outputs and perform analysis**<br/>
   Local files dimensional table analysis<br/>
   **3a. Sample Code for Users table analysis**<br/>
   users_data_df = spark.read.parquet(os.path.join(output_data, 'users/*.parquet'))<br/>
   print ('Number of Users Table rows : ' + str(users_data_df.count()))<br/>
   users_data_df.groupby('level', 'gender').count().show()<br/><br/>
   Number of Users Table rows : 104 <br/>
   
   |  Level | Gender | Count |
   |--------|--------|-------|
   | free   |   M    |   37  |
   | paid   |   M    |    7  |
   | paid   |   F    |   15  |
   | free   |   F    |   45  |
   
   **3b. Analysis Results from local files**<br/>
      Female Users : 60    Male Users : 44<br/>
      Paid Users : 22      Free Users : 82<br/><br/>
    **Sparkify needs to come up with strategies to increase paid users.**<br/>
    Check the **check-parquet-output** notebook for additional analysis
   
   ### References
   1. https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.monotonically_increasing_id.html<br/>

   2. https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html<br/>
      Pyspark: GroupBy and Aggregate Functions<br/>

   3. https://support.datavirtuality.com/hc/en-us/articles/360025526471-Query-Parquet-Files-in-Data-Virtuality-using-Amazon-Athena<br/>
   Query Parquet Files in Data Virtuality Using Amazon Athena<br/>

   4. https://www.cloudforecast.io/blog/using-parquet-on-athena-to-save-money-on-aws/<br/>
     Using Parquet On Amazon Athena For AWS Cost Optimization

   5. https://datagy.io/pandas-sort-values/<br/>
      How to Sort Data in a Pandas Dataframe (with Examples)<br/>
      by Nik  September 7, 2020<br/>

   6. aws-bucket creation using boto3<br/>
      https://stackoverflow.com/questions/31092056/how-to-create-a-s3-bucket-using-boto3<br/>
      
   7. https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/<br/>
   
   8. https://routdeepak.medium.com/writing-to-aws-s3-from-spark-91e85d09724b <br/>
      Writing to AWS S3 from Spark by Deepak Rout<br/>
      **This contains algorithms and libraries to optimize Spark session on S3**
